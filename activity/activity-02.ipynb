{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a316ff95-adc4-4871-830e-99d17fb0b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c072a1d-eb94-437d-935d-f29f852f0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import getpass\n",
    "from typing import Iterable, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680e9220-aaeb-42e6-b303-e92d72e5e639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your API key: \n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "_API_KEY = getpass.getpass(\"Your API key: \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a4981bb-3b96-4e80-8392-08d9a68c7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai = OpenAI(api_key=_API_KEY)\n",
    "client = instructor.from_openai(open_ai, mode=instructor.Mode.TOOLS_STRICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb644017-dd8c-4821-bed9-fd63e7f9dc50",
   "metadata": {},
   "source": [
    "## S2A (System 2 Attention)\n",
    "Two steps\n",
    "* Ask to rewrite the prompt to remove any unnecessary information\n",
    "* Use the rewritten prompt for the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcefb89b-4def-44fd-8c2c-4c995712959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation following the  Instructor documentation\n",
    "# https://python.useinstructor.com/prompting/zero_shot/s2a/\n",
    "\n",
    "\n",
    "class FirstStep(BaseModel):\n",
    "    relevant_context: str\n",
    "    user_query: str\n",
    "\n",
    "\n",
    "class SecondStep(BaseModel):\n",
    "    answer: int\n",
    "\n",
    "\n",
    "def rewrite_prompt(query):\n",
    "    rewritten_prompt = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=FirstStep,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                    Given the following text by a user, extract the part\n",
    "                    that is actually relevant to their question. Please\n",
    "                    include the actual question or query that the user\n",
    "                    is asking.\n",
    "\n",
    "                    Text by user:\n",
    "                    {query}\n",
    "                    \"\"\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return rewritten_prompt\n",
    "\n",
    "\n",
    "def generate_final_response(rewritten_prompt):\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=SecondStep,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"{rewritten_prompt.relevant_context}\n",
    "                    Question: {rewritten_prompt.user_query}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a28d089-0b39-485b-afc6-df3548525531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megan has 5 pieces of candy. Mary has 3 times as much candy as Megan, and then adds 10 more pieces of candy.\n",
      "How many pieces of candy does Mary have in total?\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Mary has 3 times as much candy as Megan.\n",
    "        Mary then adds 10 more pieces of candy to her collection.\n",
    "        Max is 5 years older than Mary.\n",
    "        If Megan has 5 pieces of candy, how many does Mary have in total?\n",
    "        \"\"\"\n",
    "\n",
    "# Step 1: Rewrite the prompt\n",
    "rewritten_prompt = rewrite_prompt(query)\n",
    "print(rewritten_prompt.relevant_context)\n",
    "\n",
    "print(rewritten_prompt.user_query)\n",
    "# > how many does Mary have in total?\n",
    "\n",
    "# Step 2: Generate the final response\n",
    "final_response = generate_final_response(rewritten_prompt)\n",
    "print(final_response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f141a71-3e86-49c9-9ec2-3394d83a230f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a54b27c0-4af6-4b0b-a4f2-7b4ac3bd77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psu_query = \"\"\"As Pennsylvania's only land-grant university, Penn State \n",
    "           has a broad mission of teaching, research, and public service. \n",
    "           But that mission was not so grandly conceived in 1855, when the \n",
    "           Commonwealth chartered it as a college of agricultural science \n",
    "           to apply scientific principles to farming.\n",
    "\n",
    "           Centre County became the site of the new college in response to a\n",
    "           gift of 200 acres from gentleman farmer and ironmaster James Irvin of Bellefonte. \n",
    "           Founding President Evan Pugh drew on the scientific education he had \n",
    "           received in Europe to plan a curriculum that combined theoretical\n",
    "           studies with practical applications.\n",
    "\n",
    "           Planning for a new academic initiative in the information sciences was \n",
    "           introduced in 1997, and the School of Information Sciences and Technology was\n",
    "           approved by Penn State's Board of Trustees soon after on September 11, 1998.\n",
    "           \n",
    "           The school opened its doors at Penn State's University Park campus on August 25, 1999,\n",
    "           to 105 students and five full-time faculty members, who led 43 new courses.\n",
    "           In total, 428 students enrolled in IST programs at 19 \n",
    "           Penn State campuses across the state.\n",
    "\n",
    "           Who was the first president of Penn State?\n",
    "           \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75928614-a5f4-4ad5-9904-0b4de747151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Pennsylvania's only land-grant university, Penn State has a broad mission of teaching, research, and public service. But that mission was not so grandly conceived in 1855, when the Commonwealth chartered it as a college of agricultural science to apply scientific principles to farming. Centre County became the site of the new college in response to a gift of 200 acres from gentleman farmer and ironmaster James Irvin of Bellefonte. Founding President Evan Pugh drew on the scientific education he had received in Europe to plan a curriculum that combined theoretical studies with practical applications. Planning for a new academic initiative in the information sciences was introduced in 1997, and the School of Information Sciences and Technology was approved by Penn State's Board of Trustees soon after on September 11, 1998. The school opened its doors at Penn State's University Park campus on August 25, 1999, to 105 students and five full-time faculty members, who led 43 new courses. In total, 428 students enrolled in IST programs at 19 Penn State campuses across the state.\n",
      "Who was the first president of Penn State?\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Rewrite the prompt\n",
    "rewritten_prompt = rewrite_prompt(query)\n",
    "print(rewritten_prompt.relevant_context)\n",
    "\n",
    "print(rewritten_prompt.user_query)\n",
    "# > how many does Mary have in total?\n",
    "\n",
    "# Step 2: Generate the final response\n",
    "final_response = generate_final_response(rewritten_prompt)\n",
    "print(final_response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d12a2-66e8-4b59-a8b9-ea9c5d2be672",
   "metadata": {},
   "source": [
    "# TODO: Fix the error\n",
    "\n",
    "Hint: What's the expected type in the `response_model` for `generate_final_response`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a9dec-b4ad-486c-92de-d036d94ff071",
   "metadata": {},
   "source": [
    "# Self-Ask\n",
    "\n",
    "Steps:\n",
    "* decide if follow-up questions are required\n",
    "* generate the follow-up questions\n",
    "* answer the follow-up questions\n",
    "* answer the main query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05f14355-7533-4f1a-88be-6ecf2330cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "question='When was superconductivity discovered?' answer='Superconductivity was discovered on April 8, 1911.'\n",
      "question='Who was the president of the U.S. in 1911?' answer='William Howard Taft was the president of the U.S. in 1911.'\n",
      "William Howard Taft was the president of the U.S. when superconductivity was discovered.\n"
     ]
    }
   ],
   "source": [
    "# Implementation following the `Instructor` documentation\n",
    "# https://python.useinstructor.com/prompting/zero_shot/self_ask/\n",
    "\n",
    "\n",
    "class FollowUp(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class SelfAskResponse(BaseModel):\n",
    "    is_followup_required: bool\n",
    "    follow_ups: list[FollowUp]\n",
    "    final_answer: str\n",
    "\n",
    "\n",
    "def self_ask(query):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=SelfAskResponse,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"Query: {query}\n",
    "                        Are follow-up questions needed?\n",
    "                        If so, generate follow-up questions, their answers, and then the final answer to the query.\n",
    "                        \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "query = \"Who was president of the U.S. when superconductivity was discovered?\"\n",
    "\n",
    "response = self_ask(query)\n",
    "\n",
    "print(response.is_followup_required)\n",
    "for follow_up in response.follow_ups:\n",
    "    print(follow_up)\n",
    "\n",
    "print(response.final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc5c2ffe-07f2-469e-b9ee-87ef0db5317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "question='How many Penn State campuses are there?' answer='There are 24 Penn State campuses.'\n",
      "question='What are the campuses with the longest distance between them?' answer='The campuses with the longest distance between them are Penn State University Park and Penn State Berks.'\n",
      "question='What is the distance between Penn State University Park and Penn State Berks?' answer='The distance between Penn State University Park and Penn State Berks is approximately 124 miles.'\n",
      "The longest distance between two Penn State campuses is approximately 124 miles, specifically between Penn State University Park and Penn State Berks.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the longest distance between two Penn State campuses?\"\n",
    "\n",
    "response = self_ask(query)\n",
    "\n",
    "print(response.is_followup_required)\n",
    "for follow_up in response.follow_ups:\n",
    "    print(follow_up)\n",
    "\n",
    "print(response.final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f253d-f95f-4c3d-84f3-e99da4341da4",
   "metadata": {},
   "source": [
    "# Few-shot prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1092e42c-236b-43ec-b732-c32bda0cd777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "class SentimentLabel(BaseModel):\n",
    "    sentiment: str\n",
    "\n",
    "\n",
    "def few_shot_sentiment_analysis(query):\n",
    "    # examples are from https://arxiv.org/pdf/2202.12837\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=SentimentLabel,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"Circulation revenue has increased by 5% in Finland: positive\n",
    "                            Panostaja did not disclose the purchase price: neutral\n",
    "                            Paying off the national debt will be extremely painful: negative\n",
    "                            {query}:\n",
    "                        \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "query = \"I did not think the movie was bad\"\n",
    "response = few_shot_sentiment_analysis(query)\n",
    "print(response.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbd6b9-b61e-4660-bb03-5d57cd8053dc",
   "metadata": {},
   "source": [
    "# TODO: 3-shot prompting to retrieve the org name given its mascot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc83ff-51b3-40e9-b18a-50be2847972f",
   "metadata": {},
   "source": [
    "# Analogical prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05751cdc-6706-4f0a-ab51-a9be8875065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"problem_explanation\": \"Identifying the countries in South Asia and their respective GDP per capita figures. This involves collecting data from reliable economic sources or databases to find the relevant statistics for each country in the region.\",\n",
      "  \"solution\": \"By researching the GDP per capita of countries in South Asia, we can compare the figures and determine their rankings.\"\n",
      "}\n",
      "{\n",
      "  \"problem_explanation\": \"Understanding the economic factors that influence GDP per capita, such as population size, economic development, and income distribution. This helps in contextualizing the GDP per capita figures for countries in South Asia.\",\n",
      "  \"solution\": \"Examining the socio-economic context of each country allows for a better understanding of why certain countries have higher GDP per capita than others.\"\n",
      "}\n",
      "{\n",
      "  \"problem_explanation\": \"Exploring the differences in GDP per capita across different regions and how it affects the overall economic standing of countries within South Asia. This will involve comparing South Asian countries with other countries in the world.\",\n",
      "  \"solution\": \"Comparative analysis of South Asian countries with global economic data to see how their GDP per capita stacks up against other nations.\"\n",
      "}\n",
      "{\n",
      "  \"problem_explanation\": \"Determining which country has the third largest GDP per capita in South Asia involves analyzing the GDP per capita rankings of the countries within the region, which includes India, Pakistan, Bangladesh, Nepal, Sri Lanka, Bhutan, and the Maldives.\",\n",
      "  \"solution\": \"After gathering the latest GDP per capita figures for these South Asian countries, we can rank them and identify the country that stands in the third position.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class RelevantProblem(BaseModel):\n",
    "    problem_explanation: str\n",
    "    solution: str\n",
    "\n",
    "\n",
    "class AnswersToRelevantProblems(BaseModel):\n",
    "    relevant_problems: list[RelevantProblem]\n",
    "    answer: RelevantProblem\n",
    "\n",
    "\n",
    "def analogical_prompting(query: str):\n",
    "    return client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                <problem>\n",
    "                {query}\n",
    "                </problem>\n",
    "\n",
    "                Relevant Problems: Recall three relevant and\n",
    "                distinct problems. For each problem, describe\n",
    "                it and explain the solution before solving\n",
    "                the problem\n",
    "                \"\"\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=AnswersToRelevantProblems,\n",
    "    )\n",
    "\n",
    "\n",
    "query = \"Which country has the third largest GDP per capita in South Asia?\"\n",
    "response = analogical_prompting(query)\n",
    "for problem in response.relevant_problems:\n",
    "    print(problem.model_dump_json(indent=2))\n",
    "\n",
    "print(response.answer.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e55304-bf0a-483d-8d20-1887a8e45497",
   "metadata": {},
   "source": [
    "# Tabular Chain Of Thought (Tab-CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97ebf819-79a0-43c2-a96a-070c55e03348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": [\n",
      "    {\n",
      "      \"step\": 1,\n",
      "      \"subquestion\": \"How many computers were initially in the server room?\",\n",
      "      \"procedure\": \"Identify the initial number of computers mentioned in the context.\",\n",
      "      \"result\": \"There were 9 computers initially.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 2,\n",
      "      \"subquestion\": \"How many days were computers installed?\",\n",
      "      \"procedure\": \"Count the days from Monday to Thursday.\",\n",
      "      \"result\": \"Computers were installed for 4 days.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 3,\n",
      "      \"subquestion\": \"How many computers were added each day?\",\n",
      "      \"procedure\": \"Identify the number of computers installed each day from the context.\",\n",
      "      \"result\": \"5 computers were installed each day.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 4,\n",
      "      \"subquestion\": \"What is the total number of computers installed?\",\n",
      "      \"procedure\": \"Multiply the number of days by the number of computers added each day.\",\n",
      "      \"result\": \"4 days * 5 computers/day = 20 computers installed.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 5,\n",
      "      \"subquestion\": \"What is the total number of computers now in the server room?\",\n",
      "      \"procedure\": \"Add the initial number of computers to the total number of computers installed.\",\n",
      "      \"result\": \"9 initial computers + 20 computers installed = 29 computers total.\"\n",
      "    }\n",
      "  ],\n",
      "  \"correct_answer\": 29\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class ReasoningStep(BaseModel):\n",
    "    step: int\n",
    "    subquestion: str\n",
    "    procedure: str\n",
    "    result: str\n",
    "\n",
    "\n",
    "class TabCoTResponse(BaseModel):\n",
    "    reasoning: list[ReasoningStep]\n",
    "    correct_answer: int\n",
    "\n",
    "\n",
    "def generate_structured_reasoning_response(query: str, context: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=TabCoTResponse,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"developer\",\n",
    "                \"content\": f\"\"\"\n",
    "                <system>\n",
    "                    <role>expert Question Answering system</role>\n",
    "                    <instruction>Make sure to output your reasoning in structured reasoning steps before generating a response to the user's query.</instruction>\n",
    "                </system>\n",
    "\n",
    "                <context>\n",
    "                    {context}\n",
    "                </context>\n",
    "\n",
    "                <query>\n",
    "                    {query}\n",
    "                </query>\n",
    "                \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "query = \"How many computers are now in the server room?\"\n",
    "\n",
    "context = \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday.\"\"\"\n",
    "\n",
    "response = generate_structured_reasoning_response(query, context)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85bcf1-cf3e-4349-8d1e-0608b7fe71d3",
   "metadata": {},
   "source": [
    "# Few-shot Contrastive Chain Of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99f0e86e-d75a-4663-a0e1-1edbd56c3c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chain_of_thought\": \"James writes a 3-page letter to 2 friends twice a week. This means he writes a total of 3 pages * 2 friends = 6 pages per week. In a year, there are 52 weeks, so James writes 6 pages/week * 52 weeks/year = 312 pages in a year.\",\n",
      "  \"correct_answer\": \"312 pages\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class ChainOfThought(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    correct_answer: str\n",
    "\n",
    "\n",
    "def contrastive_chain_of_thought(\n",
    "    query: str,\n",
    "    context: str,\n",
    "    example_prompt: str,\n",
    "    correct_examples: list[str],\n",
    "    incorrect_examples: list[str],\n",
    "):\n",
    "    correct_example_prompt = \"\\n\".join(\n",
    "        [f\"<Explanation>{example}</Explanation>\" for example in correct_examples]\n",
    "    )\n",
    "    incorrect_example_prompt = \"\\n\".join(\n",
    "        [\n",
    "            f\"<WrongExplanation>{example}</WrongExplanation>\"\n",
    "            for example in incorrect_examples\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=ChainOfThought,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"\n",
    "                <prompt>\n",
    "                <role>system</role>\n",
    "                <context>\n",
    "                You are an expert question answering AI System.\n",
    "\n",
    "                You are about to be given some examples of incorrect\n",
    "                and correct reasoning for a question. You will then\n",
    "                be asked to correctly reason through another question\n",
    "                to generate a valid response.\n",
    "                </context>\n",
    "\n",
    "                <question>{example_prompt}</question>\n",
    "\n",
    "                <Explanations>\n",
    "                    {correct_example_prompt}\n",
    "                    {incorrect_example_prompt}\n",
    "                </Explanations>\n",
    "                <context>{context}</context>\n",
    "                <question>{query}</question>\n",
    "\n",
    "                </prompt>\n",
    "            \"\"\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "James writes a 3-page letter to 2\n",
    "different friends twice a week.\n",
    "\"\"\"\n",
    "query = \"How many pages does James write in a year?\"\n",
    "\n",
    "sample_question = \"\"\"\n",
    "James has 30 teeth. His dentist drills 4\n",
    "of them and caps 7 more teeth than he drills.\n",
    "\n",
    "What percentage of James' teeth does the dentist fix?\n",
    "\"\"\"\n",
    "\n",
    "incorrect_examples = [\n",
    "    \"\"\"James has 30 teeth. The dentist drills and caps some\n",
    "    teeth. Since drills are normally used on cars and not\n",
    "    teeth, it's safe to say none of the teeth were actually\n",
    "    fixed.\"\"\",\n",
    "    \"\"\"The dentist drills 4 teeth and caps 11 of them, which\n",
    "    means that he fixes 15 teeth. So we take 15 and multiply\n",
    "    it by the number of petals on a daisy, and the result is\n",
    "    30%, which is the percentage of teeth he fixes.\"\"\",\n",
    "]\n",
    "\n",
    "correct_examples = [\n",
    "    \"\"\"The dentist drills 4 teeth, so there are 30 - 4 = 26\n",
    "    teeth left. The dentist caps 7 more teeth than he drills,\n",
    "    so he caps 4 + 7 = 11 teeth. Therefore, the dentist fixes\n",
    "    a total of 4 + 11 = 15 teeth. To find the percentage of\n",
    "    teeth the dentist fixes, we divide the number of teeth\n",
    "    fixed by the total number of teeth and multiply by 100:\n",
    "    15/30 x 100 = 50%\"\"\"\n",
    "]\n",
    "\n",
    "response = contrastive_chain_of_thought(\n",
    "    query=query,\n",
    "    context=context,\n",
    "    example_prompt=sample_question,\n",
    "    correct_examples=correct_examples,\n",
    "    incorrect_examples=incorrect_examples,\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f97ab-5f62-4f3f-82d6-b6372c2fb3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
